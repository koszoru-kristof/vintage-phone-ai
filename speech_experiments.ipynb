{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open-AI based chat serivce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_completion(\"How are you today? answer in a few words.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def get_transcript(audio_file_path):\n",
    "    audio_file = open(audio_file_path, \"rb\")\n",
    "    transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file, \n",
    "    response_format=\"text\"\n",
    "    )\n",
    "    return transcript\n",
    "\n",
    "def text_to_speech(text, output_file_path, voice=\"alloy\"):\n",
    "    response = client.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=voice,\n",
    "        input=text\n",
    "    )\n",
    "    response.stream_to_file(output_file_path)\n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"Hello, how are you?\"\n",
    "text_to_speech(test_sentence, \"recordings/assistant/audio.mp3\")\n",
    "\n",
    "get_transcript(\"recordings/assistant/audio.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "def record_audio(file_name, duration=5, rate=44100, chunk=1024, format=pyaudio.paInt16, channels=1):\n",
    "    audio = pyaudio.PyAudio()\n",
    "\n",
    "    stream = audio.open(format=format,\n",
    "                        channels=channels,\n",
    "                        rate=rate,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=chunk)\n",
    "\n",
    "    print(\"Recording...\")\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for i in range(0, int(rate / chunk * duration)):\n",
    "        data = stream.read(chunk)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"Finished recording.\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    wave_file = wave.open(file_name, 'wb')\n",
    "    wave_file.setnchannels(channels)\n",
    "    wave_file.setsampwidth(audio.get_sample_size(format))\n",
    "    wave_file.setframerate(rate)\n",
    "    wave_file.writeframes(b''.join(frames))\n",
    "    wave_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_audio(\"audio.wav\", duration=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's put it all together\n",
    "# First, we'll record audio to a file\n",
    "# Then, we'll transcribe the audio\n",
    "# Finally, we'll send the transcript to the LLM\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "from playsound import playsound\n",
    "\n",
    "\n",
    "# Get the current time\n",
    "current_time = datetime.datetime.now()\n",
    "\n",
    "# Format the current time as a string\n",
    "filename = current_time.strftime(\"%Y%m%d-%H%M%S\") + \".wav\"\n",
    "dir = \"recordings/user\"\n",
    "rec_full_path = os.path.join(dir, filename)\n",
    "\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "\n",
    "dir_assistant = \"recordings/assistant\"\n",
    "if not os.path.exists(dir_assistant):\n",
    "    os.makedirs(dir_assistant)\n",
    "\n",
    "# Record audio to the file\n",
    "record_audio(rec_full_path, duration=5)\n",
    "\n",
    "# Transcribe the audio\n",
    "user_query_text = get_transcript(rec_full_path)\n",
    "print(\"user:\", user_query_text)\n",
    "\n",
    "# Send it to the LLM see what it says\n",
    "response_assistant_text = get_completion(user_query_text)\n",
    "print(\"assistant:\", response_assistant_text)\n",
    "\n",
    "# Convert the response to speech\n",
    "response_assitant_filename = current_time.strftime(\"%Y%m%d-%H%M%S\") + \".mp3\"\n",
    "response_assistant_full_path = os.path.join(dir_assistant, response_assitant_filename)\n",
    "\n",
    "audio_response_assistant = text_to_speech(response_assistant_text, response_assistant_full_path)\n",
    "\n",
    "# Play the response\n",
    "playsound(audio_response_assistant)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
